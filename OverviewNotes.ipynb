{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "japanese-speed",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-aquatic",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    "- Extract entities\n",
    "- Topic modelling\n",
    "- Knowledge search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-bedroom",
   "metadata": {},
   "source": [
    "### Algorthmic building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-analyst",
   "metadata": {},
   "source": [
    "### Textual data\n",
    "\n",
    "Read textual data and add structured metadata using Dependency parsing -> POS-tagging.\n",
    "\n",
    "Subj -> Verb <- Object \n",
    "\n",
    "This (nsubj) is (verb) a (det) sentence (attr)\n",
    "\n",
    "### Semantic triple\n",
    "\n",
    "(Subject, Predicate, Object) - RDF- w3C\n",
    "\n",
    "\n",
    "## Preprocessing data\n",
    "\n",
    "Criteria for good dataset\n",
    "\n",
    "- Long well written texts covering divers topics\n",
    "- Extensive and well maintained\n",
    "- Available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-carbon",
   "metadata": {},
   "source": [
    "### Eploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd()\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "data_file = root_dir / 'data' / 'wiki_movie_plots_deduped.csv'\n",
    "movie_plots_data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-pocket",
   "metadata": {},
   "source": [
    "### country of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots_data.groupby(['Origin/Ethnicity']).size().sort_values(ascending=True).plot.barh(figsize=(4,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most movies are American/ British - western - Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-coverage",
   "metadata": {},
   "source": [
    "### Visualize release year trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots_data.groupby([\"Release Year\"]).size().plot(kind='bar', figsize=(22,4), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-indicator",
   "metadata": {},
   "source": [
    "### Visualize genre breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots_data[movie_plots_data['Genre'] != 'unknown'].groupby(['Genre']).size().sort_values(ascending=True).tail(25).plot.barh(figsize=(4,8), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ignore both ('unknown'/ 'Unknown') types\n",
    "# Suggests exponential distribution on num of movies per genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-mixture",
   "metadata": {},
   "source": [
    "### Visualize director movie count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots_data[movie_plots_data['Director'] != 'Unknown'].groupby(['Director']).size().sort_values(ascending=True).tail(25).plot.barh(figsize=(4,8), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-patrol",
   "metadata": {},
   "source": [
    "### Get subset of data released after 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = movie_plots_data[movie_plots_data['Release Year'] >= 2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-framework",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-alloy",
   "metadata": {},
   "source": [
    "### NLP data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modelling used for knowledge mining\n",
    "# looks for ways to group text into clusters"
   ]
  },
  {
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Select films newer than 2015 and of comedy genre"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots = movie_plots_data.loc[\n",
    "    (movie_plots_data['Release Year'] >= 2015) &\n",
    "     (movie_plots_data['Genre'].str.contains(\"comedy\"))\n",
    "].Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        # remove stopword tokens and token of len < 3\n",
    "        if token not in STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = movie_plots.map(preprocess)\n",
    "preprocessed_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare sentences before and after preprocessing\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(movie_plots.head(1))\n",
    "    display(preprocessed_docs.head(1))"
   ]
  },
  {
   "source": [
    "### Create bag of words\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out extreme values\n",
    "# Words that appear in less than 10 doc\n",
    "# appear in more than 50% of docs - Can see link with TF-IDF ideas here\n",
    "# keeping first 100,000 tokens sorted by appearance frequency\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create bow corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "\n",
    "print(len(bow_corpus))\n",
    "preprocessed_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# count word occurence\n",
    "word_dict_count = {}\n",
    "for doc in bow_corpus:\n",
    "    for i, word_info in enumerate(doc):\n",
    "        word = dictionary[word_info[0]]\n",
    "        print(word, word_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}